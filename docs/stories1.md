# ðŸ§µ Narrative-Driven ML Curriculum

Inspired by *GÃ¶del, Escher, Bach*, this curriculum weaves machine learning concepts into a continuous story, where each section emerges from the last.

---

## 1. Linear Regression

- Our simplest lens to see the world: input â†¦ output.
- Continuous loss (MSE) teaches us the idea of optimizing a model.
- Introduces **overfitting** via the temptation to fit every data point.

---

## 2. Overfitting

- Too perfect a fit spoils the truth â€” complexity hides generalization.
- Gives rise to the **need for validation** and holding out data.

---

## 3. Train/Test Split

- To test our faith in generalization, we must withhold what we know.
- Shows how to measure learning, not just memory.
- Sets the stage for evaluating not just *how*, but *how well*.

---

## 4. Precision vs Recall

- In choosing what's *important* to get right, we define our values.
- A tension emerges: do we cry wolf or miss the threat?
- Contextualizes error into **decision-making under uncertainty**.

---

## 5. Specificity vs Sensitivity

- In domains like medicine, these values become life and death.
- Mirrors precision/recall but emphasizes the *perspective of absence* (specificity).

---

## 6. Categorical vs Continuous Loss

- We ask: what are we *measuring*? Numbers or categories?
- Explores **how models learn differently** depending on output space.

---

## 7. Decision Trees

- A more **human-like model**: if-then paths, like rules of logic.
- Explores the *structure* of decisions.
- Overfitting reappears â€” trees can memorize too well.

---

## 8. Boosting / Bagging

- The wisdom of crowds: weak learners made strong.
- Emphasizes diversity, randomness, and averaging as a **meta-strategy**.
- A solution to variance and overfitting from a systems perspective.

---

## 9. Neural Networks (Simple Approach)

- A leap: from rules to **layers of abstract representation**.
- Continuous loss meets nonlinearity, and optimization deepens.
- Even the simplest NN echoes linear regression â€” with depth.

---

## 10. Seasonality

- Time reenters the picture: patterns not in space, but in sequence.
- Models must now remember, wait, and revisit.
- Introduces temporal modeling and **domain-aware thinking**.

---

# ðŸŒ€ Narrative Flow Principles

- Each section **ends with a question** or concept the next chapter answers.
- Recurrent motifs: overfitting, optimization, abstraction.
- Story: we go from **simple to powerful**, but **always come back to interpretability and purpose**.
