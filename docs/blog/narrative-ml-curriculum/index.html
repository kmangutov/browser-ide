<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="A continuous story-like approach to teaching machine learning concepts, inspired by G√∂del, Escher, Bach."><title>Narrative-Driven ML Curriculum</title><style>.pyodide-embed[data-astro-cid-udmswuc5]{margin:2rem 0;border:1px solid #e0e0e0;border-radius:6px;overflow:hidden}.code-editor[data-astro-cid-udmswuc5]{padding:1rem;background-color:#282c34;overflow:auto;max-height:400px}.code-display[data-astro-cid-udmswuc5]{margin:0;font-family:Consolas,Monaco,Courier New,monospace;font-size:14px;line-height:1.5;color:#abb2bf}.controls[data-astro-cid-udmswuc5]{padding:.5rem;background-color:#f8f9fa;border-bottom:1px solid #e0e0e0}#runButton[data-astro-cid-udmswuc5]{background-color:#3498db;color:#fff;border:none;padding:8px 16px;border-radius:4px;cursor:pointer;font-size:14px;transition:background-color .2s}#runButton[data-astro-cid-udmswuc5]:hover{background-color:#2980b9}#runButton[data-astro-cid-udmswuc5]:disabled{background-color:#95a5a6;cursor:not-allowed}.output-container[data-astro-cid-udmswuc5]{padding:1rem;background-color:#f8f9fa;border-top:1px solid #e0e0e0;min-height:100px}.output[data-astro-cid-udmswuc5]{font-family:Consolas,Monaco,Courier New,monospace;font-size:14px;white-space:pre-wrap}.error[data-astro-cid-udmswuc5]{color:#e74c3c;font-weight:700}.warning[data-astro-cid-udmswuc5]{color:#f39c12;font-weight:700}.note[data-astro-cid-udmswuc5]{margin-top:1rem;padding:.5rem;background-color:#edf2f7;border-left:4px solid #3498db;font-size:.9rem}
body{font-family:Segoe UI,Tahoma,Geneva,Verdana,sans-serif;max-width:1000px;margin:0 auto;padding:20px;line-height:1.6;color:#333}.main-nav[data-astro-cid-37fxchfa]{background-color:#f8f9fa;border-radius:8px;margin-bottom:20px;box-shadow:0 2px 4px #0000001a}.main-nav[data-astro-cid-37fxchfa] ul[data-astro-cid-37fxchfa]{list-style:none;padding:0;margin:0;display:flex}.main-nav[data-astro-cid-37fxchfa] li[data-astro-cid-37fxchfa]{padding:0}.main-nav[data-astro-cid-37fxchfa] a[data-astro-cid-37fxchfa]{display:block;padding:15px 20px;text-decoration:none;color:#333;font-weight:500}.main-nav[data-astro-cid-37fxchfa] a[data-astro-cid-37fxchfa]:hover{background-color:#e9ecef}.main-nav[data-astro-cid-37fxchfa] a[data-astro-cid-37fxchfa].active{background-color:#3498db;color:#fff}.content[data-astro-cid-37fxchfa]{background-color:#fff;padding:2rem;border-radius:8px;box-shadow:0 2px 4px #0000001a}pre[data-astro-cid-37fxchfa]{background-color:#f8f9fa;padding:1rem;border-radius:4px;overflow-x:auto}
article[data-astro-cid-2q5oecfc]{margin-bottom:2rem}header[data-astro-cid-2q5oecfc]{margin-bottom:2rem}h1[data-astro-cid-2q5oecfc]{margin-bottom:.5rem;color:#2c3e50}time[data-astro-cid-2q5oecfc]{color:#7f8c8d;font-size:.9rem}.prose[data-astro-cid-2q5oecfc]{line-height:1.8}.prose[data-astro-cid-2q5oecfc] h2[data-astro-cid-2q5oecfc]{margin-top:2rem;color:#2c3e50}.prose[data-astro-cid-2q5oecfc] p[data-astro-cid-2q5oecfc]{margin:1rem 0}.prose[data-astro-cid-2q5oecfc] img[data-astro-cid-2q5oecfc]{max-width:100%;border-radius:4px}.prose[data-astro-cid-2q5oecfc] .demo-container[data-astro-cid-2q5oecfc]{background-color:#f8f9fa;border-radius:4px;padding:1rem;margin:1.5rem 0}
</style><script type="module">let a;function f(){return a||(a=async function(){if(!window.loadPyodide){const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/pyodide/v0.25.0/full/pyodide.js",document.head.appendChild(t),await new Promise(l=>t.onload=l)}console.log("Loading Pyodide...");const d=await window.loadPyodide();return console.log("Pyodide loaded successfully!"),console.log("Loading scikit-learn, numpy, and pandas..."),await d.loadPackage(["scikit-learn","numpy","pandas"]),console.log("ML packages loaded successfully!"),d}()),a}document.addEventListener("DOMContentLoaded",()=>{document.querySelectorAll(".pyodide-embed").forEach((t,l)=>{const s=t.querySelector("#runButton"),e=t.querySelector("#output"),p=t.querySelector(".code-display");let u=f();e.innerText="Python environment loading...",s.addEventListener("click",async()=>{s.disabled=!0,e.innerText="Running...";try{const o=await u,i=p.textContent;let r="";o.setStdout({write:c=>{r+=c,e.innerText=r}}),o.setStderr({write:c=>{r+=`
Error: ${c}`,e.innerText=r}}),r="";const n=await o.runPythonAsync(i);n&&typeof n=="object"&&"error"in n?(e.innerHTML=`<span class="error">Error: ${n.error}</span>`,console.error("Python error:",n.error)):n!=null&&r===""&&(typeof n=="object"?e.innerText=JSON.stringify(n,null,2):e.innerText=String(n))}catch(o){e.innerHTML=`<span class="error">Error: ${o.message||String(o)}</span>`,console.error("Python execution error:",o)}finally{s.disabled=!1}});const y=new IntersectionObserver(o=>{o[0].isIntersecting&&(u.then(()=>{e.innerText="Python environment ready. Click 'Run Code' to execute."}).catch(i=>{e.innerText=`Error loading Python: ${i.message}`,console.error("Failed to load Pyodide:",i)}),y.disconnect())});y.observe(t)})});
</script></head> <body data-astro-cid-37fxchfa> <!-- Navigation Bar --> <nav class="main-nav" data-astro-cid-37fxchfa> <ul data-astro-cid-37fxchfa> <li data-astro-cid-37fxchfa><a href="/" class="" data-astro-cid-37fxchfa>Playground</a></li> <li data-astro-cid-37fxchfa><a href="/blog/" class="active" data-astro-cid-37fxchfa>Blog</a></li> </ul> </nav> <main class="content" data-astro-cid-37fxchfa>  <article data-astro-cid-2q5oecfc> <header data-astro-cid-2q5oecfc> <h1 data-astro-cid-2q5oecfc>Narrative-Driven ML Curriculum</h1> <time data-astro-cid-2q5oecfc>April 19, 2023</time> </header> <div class="prose" data-astro-cid-2q5oecfc> <h1 id="-narrative-driven-ml-curriculum">üßµ Narrative-Driven ML Curriculum</h1>
<p>Inspired by <em>G√∂del, Escher, Bach</em>, this curriculum weaves machine learning concepts into a continuous story, where each section emerges naturally from the last.</p>
<h2 id="why-a-narrative-approach">Why a narrative approach?</h2>
<p>Traditional ML courses often present concepts as disconnected topics. But in reality, each idea flows from challenges encountered in previous approaches. By teaching ML as a continuous narrative, we help learners understand not just <em>how</em> techniques work, but <em>why</em> they were developed.</p>
<h2 id="1-linear-regression-our-first-model">1. Linear Regression: Our First Model</h2>
<p>The simplest lens through which we can view the world is as a mapping from input to output. Linear regression establishes this foundation.</p>
<div class="pyodide-embed" data-astro-cid-udmswuc5> <div class="code-editor" data-astro-cid-udmswuc5> <pre class="code-display" data-astro-cid-udmswuc5># Import libraries
import numpy as np
from sklearn.linear_model import LinearRegression

try:
  # Generate synthetic data directly instead of using built-in datasets
  np.random.seed(42)
  X = np.random.rand(100, 1) * 10    # 100 samples, 1 feature
  noise = np.random.randn(100, 1) * 2  # some random noise
  y = 3 * X + 5 + noise               # y = 3x + 5 + noise

  # Create and train model
  model = LinearRegression()
  model.fit(X, y)

  # Make predictions across the input range
  X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
  y_pred = model.predict(X_range)

  # Print model info
  print(f&quot;Coefficient: {model.coef_[0][0]:.4f}&quot;)
  print(f&quot;Intercept: {model.intercept_[0]:.4f}&quot;)

  # Create results dictionary - convert all NumPy arrays to lists
  result = {
      &quot;X&quot;: X.flatten().tolist(),
      &quot;y&quot;: y.flatten().tolist(),
      &quot;X_range&quot;: X_range.flatten().tolist(),
      &quot;y_pred&quot;: y_pred.flatten().tolist(),
      &quot;coefficient&quot;: float(model.coef_[0][0]),
      &quot;intercept&quot;: float(model.intercept_[0])
  }

  # Return results dictionary
  result
except Exception as e:
  # Return error information
  {&quot;error&quot;: str(e)}</pre> </div> <div class="controls" data-astro-cid-udmswuc5> <button id="runButton" data-astro-cid-udmswuc5>Run Code</button> </div> <div class="output-container" data-astro-cid-udmswuc5> <div id="output" class="output" data-astro-cid-udmswuc5></div> </div> </div>  
<p>The continuous loss function (Mean Squared Error) teaches us the fundamental idea of optimizing a model. But it also introduces the temptation to fit every data point perfectly, leading us to our next challenge‚Ä¶</p>
<h2 id="2-overfitting-the-danger-of-perfection">2. Overfitting: The Danger of Perfection</h2>
<p>Too perfect a fit spoils the truth. When a model captures not just patterns but noise, it loses its generalization power.</p>
<div class="pyodide-embed" data-astro-cid-udmswuc5> <div class="code-editor" data-astro-cid-udmswuc5> <pre class="code-display" data-astro-cid-udmswuc5># Import libraries
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

try:
  # Generate synthetic data
  np.random.seed(42)
  X = np.sort(np.random.rand(20) * 5).reshape(-1, 1)
  y = np.sin(X).ravel() + np.random.randn(20) * 0.2

  # Create models with different polynomial degrees
  degrees = [1, 3, 10]
  models = {}
  predictions = {}

  for degree in degrees:
      # Create polynomial regression model
      model = make_pipeline(
          PolynomialFeatures(degree),
          LinearRegression()
      )
      model.fit(X, y)
      models[degree] = model
      
      # Make predictions
      X_range = np.linspace(0, 5, 100).reshape(-1, 1)
      predictions[degree] = model.predict(X_range)
      
      # Print model details
      print(f&quot;Degree {degree} polynomial:&quot;)
      train_score = model.score(X, y)
      print(f&quot;  Training R¬≤ score: {train_score:.4f}&quot;)

  # Create results dictionary - convert all NumPy arrays to lists
  result = {
      &quot;X&quot;: X.flatten().tolist(),
      &quot;y&quot;: y.tolist(),
      &quot;X_range&quot;: np.linspace(0, 5, 100).tolist(),
      &quot;predictions&quot;: {
          str(degree): predictions[degree].tolist() 
          for degree in degrees
      },
      &quot;train_scores&quot;: {
          str(degree): float(models[degree].score(X, y))
          for degree in degrees
      }
  }

  # Return results
  result
except Exception as e:
  # Return error information
  {&quot;error&quot;: str(e)}</pre> </div> <div class="controls" data-astro-cid-udmswuc5> <button id="runButton" data-astro-cid-udmswuc5>Run Code</button> </div> <div class="output-container" data-astro-cid-udmswuc5> <div id="output" class="output" data-astro-cid-udmswuc5></div> </div> </div>  
<p>This leads us to a critical insight: we need a way to measure how well our model performs on data it hasn‚Äôt seen before.</p>
<h2 id="3-traintest-split-measuring-generalization">3. Train/Test Split: Measuring Generalization</h2>
<p>To test our faith in generalization, we must withhold what we know. The train/test split is our first tool for measuring true learning rather than just memorization.</p>
<div class="pyodide-embed" data-astro-cid-udmswuc5> <div class="code-editor" data-astro-cid-udmswuc5> <pre class="code-display" data-astro-cid-udmswuc5># Import libraries
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

try:
  # Generate synthetic data
  np.random.seed(42)
  X = np.sort(np.random.rand(50) * 5).reshape(-1, 1)
  y = np.sin(X).ravel() + np.random.randn(50) * 0.2

  # Split data into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.3, random_state=42
  )

  # Create models with different polynomial degrees
  degrees = [1, 3, 15]
  model_results = {}

  for degree in degrees:
      # Create polynomial regression model
      model = make_pipeline(
          PolynomialFeatures(degree),
          LinearRegression()
      )
      model.fit(X_train, y_train)
      
      # Evaluate on training data
      y_train_pred = model.predict(X_train)
      train_mse = mean_squared_error(y_train, y_train_pred)
      train_r2 = r2_score(y_train, y_train_pred)
      
      # Evaluate on testing data
      y_test_pred = model.predict(X_test)
      test_mse = mean_squared_error(y_test, y_test_pred)
      test_r2 = r2_score(y_test, y_test_pred)
      
      # Print results
      print(f&quot;Degree {degree} polynomial:&quot;)
      print(f&quot;  Training MSE: {train_mse:.4f}, R¬≤: {train_r2:.4f}&quot;)
      print(f&quot;  Testing MSE: {test_mse:.4f}, R¬≤: {test_r2:.4f}&quot;)
      
      # Store results - convert NumPy arrays to lists
      model_results[degree] = {
          &quot;train_mse&quot;: float(train_mse),
          &quot;train_r2&quot;: float(train_r2),
          &quot;test_mse&quot;: float(test_mse),
          &quot;test_r2&quot;: float(test_r2),
          &quot;y_train_pred&quot;: y_train_pred.tolist(),
          &quot;y_test_pred&quot;: y_test_pred.tolist()
      }

  # Create results dictionary - convert NumPy arrays to lists
  result = {
      &quot;X_train&quot;: X_train.flatten().tolist(),
      &quot;y_train&quot;: y_train.tolist(),
      &quot;X_test&quot;: X_test.flatten().tolist(),
      &quot;y_test&quot;: y_test.tolist(),
      &quot;degrees&quot;: degrees,
      &quot;results&quot;: model_results
  }

  # Return results
  result
except Exception as e:
  # Return error information
  {&quot;error&quot;: str(e)}</pre> </div> <div class="controls" data-astro-cid-udmswuc5> <button id="runButton" data-astro-cid-udmswuc5>Run Code</button> </div> <div class="output-container" data-astro-cid-udmswuc5> <div id="output" class="output" data-astro-cid-udmswuc5></div> </div> </div>  
<p>But what if not all errors are created equal? This leads us to consider the context in which our models operate.</p>
<h2 id="-narrative-flow-principles">üåÄ Narrative Flow Principles</h2>
<p>This curriculum follows several key principles:</p>
<ol>
<li><strong>Continuity</strong>: Each section ends with a question or concept the next chapter answers</li>
<li><strong>Recurrent Motifs</strong>: Themes like overfitting, optimization, and abstraction reappear throughout</li>
<li><strong>Journey</strong>: We progress from simple to powerful models, but always return to interpretability and purpose</li>
</ol>
<p>The remaining chapters in our narrative continue this journey:</p>
<ul>
<li><strong>Precision vs Recall</strong>: Choosing what‚Äôs important to get right</li>
<li><strong>Decision Trees</strong>: A more human-like model of if-then rules</li>
<li><strong>Ensemble Methods</strong>: The wisdom of crowds in boosting and bagging</li>
<li><strong>Neural Networks</strong>: Layers of abstract representations</li>
<li><strong>Temporal Models</strong>: Patterns not in space, but in sequence</li>
</ul>
<h2 id="next-steps">Next Steps</h2>
<p>Our curriculum is designed so each concept builds naturally on the challenges and limitations revealed in the previous section. Follow along as we continue to explore machine learning through this narrative lens.</p>
<p><a href="/blog/">Explore More Articles ‚Üí</a></p>
<hr/>
<p><em>This approach is inspired by the interconnected storytelling in Douglas Hofstadter‚Äôs ‚ÄúG√∂del, Escher, Bach‚Äù which weaves together seemingly disparate concepts into a coherent whole.</em></p> </div> </article>   </main> </body></html>