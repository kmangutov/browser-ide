<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Understanding the simplest lens through which we can see the world"><title>Linear Regression: The Beginning of Our Journey</title><link rel="stylesheet" href="/browser-ide/assets/creating-pyodide-animations-D13ptpoQ.css">
<link rel="stylesheet" href="/browser-ide/assets/creating-pyodide-animations-3o92PDWl.css"></head> <body data-astro-cid-37fxchfa> <!-- Navigation Bar --> <nav class="main-nav" data-astro-cid-37fxchfa> <ul data-astro-cid-37fxchfa> <li data-astro-cid-37fxchfa><a href="/browser-ide" class="" data-astro-cid-37fxchfa>Playground</a></li> <li data-astro-cid-37fxchfa><a href="/browser-ide/blog/" class="" data-astro-cid-37fxchfa>Blog</a></li> </ul> </nav> <main class="content" data-astro-cid-37fxchfa>  <article data-astro-cid-2q5oecfc> <header data-astro-cid-2q5oecfc> <h1 data-astro-cid-2q5oecfc>Linear Regression: The Beginning of Our Journey</h1>  </header> <div class="prose" data-astro-cid-2q5oecfc> <h1 id="linear-regression-where-our-story-begins">Linear Regression: Where Our Story Begins</h1>
<h2 id="our-simplest-lens-to-see-the-world-input--output">Our simplest lens to see the world: input ↦ output</h2>
<p>The journey of machine learning begins with a simple idea: given some input, can we predict an output?
Imagine trying to predict house prices based on their size. As the size increases, the price tends to increase as well. This relationship can be modeled as a line: y = mx + b, where y is the price, x is the size, m is the slope, and b is the y-intercept.</p>

<h2 id="continuous-loss-mse-teaches-us-the-idea-of-optimizing-a-model">Continuous loss (MSE) teaches us the idea of optimizing a model</h2>
<p>How do we know if our line is a good fit? We measure the distance between our predictions and the actual values. The most common way is through Mean Squared Error (MSE): the average of the squared differences between predictions and actual values. By minimizing this loss, we find the best-fitting line.</p>

<h2 id="introduces-overfitting-via-the-temptation-to-fit-every-data-point">Introduces overfitting via the temptation to fit every data point</h2>
<p>But wait—what if we could create a model so complex that it passes through every single data point perfectly? Wouldn’t that be better? This is where we encounter our first fundamental challenge: the tension between fitting the training data and generalizing to new, unseen data.</p>

<p>As we fit our model too precisely to our training data, we risk missing the forest for the trees. This leads us naturally to our next topic: <strong>Overfitting</strong>.</p> </div> </article>   </main> </body></html>