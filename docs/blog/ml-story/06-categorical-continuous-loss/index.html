<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="How the nature of our prediction shapes our evaluation strategies"><title>Categorical vs Continuous Loss: Measuring Different Realities</title><link rel="stylesheet" href="/browser-ide/assets/creating-pyodide-animations-D13ptpoQ.css">
<link rel="stylesheet" href="/browser-ide/assets/creating-pyodide-animations-3o92PDWl.css"></head> <body data-astro-cid-37fxchfa> <!-- Navigation Bar --> <nav class="main-nav" data-astro-cid-37fxchfa> <ul data-astro-cid-37fxchfa> <li data-astro-cid-37fxchfa><a href="/browser-ide" class="" data-astro-cid-37fxchfa>Playground</a></li> <li data-astro-cid-37fxchfa><a href="/browser-ide/blog/" class="" data-astro-cid-37fxchfa>Blog</a></li> </ul> </nav> <main class="content" data-astro-cid-37fxchfa>  <article data-astro-cid-2q5oecfc> <header data-astro-cid-2q5oecfc> <h1 data-astro-cid-2q5oecfc>Categorical vs Continuous Loss: Measuring Different Realities</h1>  </header> <div class="prose" data-astro-cid-2q5oecfc> <h1 id="categorical-vs-continuous-loss-the-nature-of-prediction">Categorical vs Continuous Loss: The Nature of Prediction</h1>
<h2 id="we-ask-what-are-we-measuring-numbers-or-categories">We ask: what are we measuring? Numbers or categories?</h2>
<p>Thus far, we’ve explored various ways to evaluate models, but we haven’t deeply examined how the nature of what we’re predicting fundamentally changes our approach. Are we predicting a discrete category (classification) or a continuous value (regression)? This distinction shapes not only our evaluation metrics but also how our models learn.</p>

<h2 id="explores-how-models-learn-differently-depending-on-output-space">Explores how models learn differently depending on output space</h2>
<p>In classification, we typically use loss functions like cross-entropy loss, which penalizes confident but wrong predictions severely. In regression, we often use mean squared error or mean absolute error, which directly measure the numerical difference between predicted and actual values. These different loss functions guide our models to learn in fundamentally different ways.</p>

<h2 id="the-bridge-between-worlds-probability-distributions">The bridge between worlds: probability distributions</h2>
<p>Behind both categorical and continuous predictions lies the concept of probability distributions. A classification model outputs a probability distribution over discrete categories, while a regression model (in its probabilistic form) outputs a continuous probability distribution. This perspective unifies our understanding and opens the door to more sophisticated modeling approaches.</p>

<p>As we’ve seen, the methods we’ve discussed so far all involve models that make predictions based on mathematical functions. But what if we could create models that make decisions based on logical rules, similar to human reasoning? This brings us to our next topic: <strong>Decision Trees</strong>.</p> </div> </article>   </main> </body></html>