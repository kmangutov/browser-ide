<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Understanding models inspired by the human brain"><title>Neural Networks: Layers of Abstraction</title><style>body{font-family:Segoe UI,Tahoma,Geneva,Verdana,sans-serif;max-width:1000px;margin:0 auto;padding:20px;line-height:1.6;color:#333}.main-nav[data-astro-cid-37fxchfa]{background-color:#f8f9fa;border-radius:8px;margin-bottom:20px;box-shadow:0 2px 4px #0000001a}.main-nav[data-astro-cid-37fxchfa] ul[data-astro-cid-37fxchfa]{list-style:none;padding:0;margin:0;display:flex}.main-nav[data-astro-cid-37fxchfa] li[data-astro-cid-37fxchfa]{padding:0}.main-nav[data-astro-cid-37fxchfa] a[data-astro-cid-37fxchfa]{display:block;padding:15px 20px;text-decoration:none;color:#333;font-weight:500}.main-nav[data-astro-cid-37fxchfa] a[data-astro-cid-37fxchfa]:hover{background-color:#e9ecef}.main-nav[data-astro-cid-37fxchfa] a[data-astro-cid-37fxchfa].active{background-color:#3498db;color:#fff}.content[data-astro-cid-37fxchfa]{background-color:#fff;padding:2rem;border-radius:8px;box-shadow:0 2px 4px #0000001a}pre[data-astro-cid-37fxchfa]{background-color:#f8f9fa;padding:1rem;border-radius:4px;overflow-x:auto}
article[data-astro-cid-2q5oecfc]{margin-bottom:2rem}header[data-astro-cid-2q5oecfc]{margin-bottom:2rem}h1[data-astro-cid-2q5oecfc]{margin-bottom:.5rem;color:#2c3e50}time[data-astro-cid-2q5oecfc]{color:#7f8c8d;font-size:.9rem}.prose[data-astro-cid-2q5oecfc]{line-height:1.8}.prose[data-astro-cid-2q5oecfc] h2[data-astro-cid-2q5oecfc]{margin-top:2rem;color:#2c3e50}.prose[data-astro-cid-2q5oecfc] p[data-astro-cid-2q5oecfc]{margin:1rem 0}.prose[data-astro-cid-2q5oecfc] img[data-astro-cid-2q5oecfc]{max-width:100%;border-radius:4px}.prose[data-astro-cid-2q5oecfc] .demo-container[data-astro-cid-2q5oecfc]{background-color:#f8f9fa;border-radius:4px;padding:1rem;margin:1.5rem 0}
</style></head> <body data-astro-cid-37fxchfa> <!-- Navigation Bar --> <nav class="main-nav" data-astro-cid-37fxchfa> <ul data-astro-cid-37fxchfa> <li data-astro-cid-37fxchfa><a href="/" class="" data-astro-cid-37fxchfa>Playground</a></li> <li data-astro-cid-37fxchfa><a href="/blog/" class="" data-astro-cid-37fxchfa>Blog</a></li> </ul> </nav> <main class="content" data-astro-cid-37fxchfa>  <article data-astro-cid-2q5oecfc> <header data-astro-cid-2q5oecfc> <h1 data-astro-cid-2q5oecfc>Neural Networks: Layers of Abstraction</h1>  </header> <div class="prose" data-astro-cid-2q5oecfc> <h1 id="neural-networks-inspired-by-nature">Neural Networks: Inspired by Nature</h1>
<h2 id="a-leap-from-rules-to-layers-of-abstract-representation">A leap: from rules to layers of abstract representation</h2>
<p>Neural networks represent a fundamental shift in how we approach machine learning. Rather than manually engineering features or specifying rules, neural networks learn hierarchical representations of the data through layers of interconnected “neurons.” Each layer transforms its input, gradually building up from simple features to complex concepts, similar to how our brains process information.</p>

<h2 id="continuous-loss-meets-nonlinearity-and-optimization-deepens">Continuous loss meets nonlinearity, and optimization deepens</h2>
<p>At their core, neural networks are mathematical functions with many parameters. However, the introduction of nonlinear activation functions allows them to learn complex patterns that linear models cannot capture. Training these models involves optimizing the loss function across many dimensions, a process made possible by the backpropagation algorithm, which efficiently calculates how each parameter affects the output.</p>

<h2 id="even-the-simplest-nn-echoes-linear-regression--with-depth">Even the simplest NN echoes linear regression — with depth</h2>
<p>It’s enlightening to see that a neural network with no hidden layers and no activation function is essentially a linear regression model. Adding depth (more layers) and nonlinearity (activation functions) transforms this simple model into a universal function approximator—capable, in theory, of learning any mapping between inputs and outputs given enough data and capacity.</p>

<p>So far, our journey has focused on models that make predictions based on static inputs. But what about data that changes over time, where the sequence of events matters? This brings us to our final topic: <strong>Seasonality</strong>.</p> </div> </article>   </main> </body></html>