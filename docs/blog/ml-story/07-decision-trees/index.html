<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Understanding models that learn through logical rules rather than mathematical functions"><title>Decision Trees: Human-Like Learning</title><link rel="stylesheet" href="/browser-ide/assets/creating-pyodide-animations-D13ptpoQ.css">
<link rel="stylesheet" href="/browser-ide/assets/creating-pyodide-animations-3o92PDWl.css"></head> <body data-astro-cid-37fxchfa> <!-- Navigation Bar --> <nav class="main-nav" data-astro-cid-37fxchfa> <ul data-astro-cid-37fxchfa> <li data-astro-cid-37fxchfa><a href="/browser-ide" class="" data-astro-cid-37fxchfa>Playground</a></li> <li data-astro-cid-37fxchfa><a href="/browser-ide/blog/" class="" data-astro-cid-37fxchfa>Blog</a></li> </ul> </nav> <main class="content" data-astro-cid-37fxchfa>  <article data-astro-cid-2q5oecfc> <header data-astro-cid-2q5oecfc> <h1 data-astro-cid-2q5oecfc>Decision Trees: Human-Like Learning</h1>  </header> <div class="prose" data-astro-cid-2q5oecfc> <h1 id="decision-trees-the-logic-of-decisions">Decision Trees: The Logic of Decisions</h1>
<h2 id="a-more-human-like-model-if-then-paths-like-rules-of-logic">A more human-like model: if-then paths, like rules of logic</h2>
<p>Unlike the mathematical models we’ve explored so far, decision trees make predictions through a series of logical if-then rules. Starting at the root, the model traverses a path of decisions based on feature values until it reaches a leaf node that provides the prediction. This process mirrors human decision-making, making decision trees highly interpretable.</p>

<h2 id="explores-the-structure-of-decisions">Explores the structure of decisions</h2>
<p>Decision trees automatically identify the most informative features and the optimal thresholds for splitting the data. At each node, the tree chooses the split that maximizes information gain—reducing uncertainty about the target variable as much as possible. This process reveals the structure of the decision-making process inherent in the data.</p>

<h2 id="overfitting-reappears--trees-can-memorize-too-well">Overfitting reappears — trees can memorize too well</h2>
<p>Just as we saw with linear models, decision trees face the challenge of overfitting. A tree with too many nodes can perfectly fit the training data by essentially memorizing it, creating branches for every minor pattern or noise in the data. This leads to poor generalization on unseen data, reintroducing the fundamental tension between model complexity and generalizability.</p>

<p>If a single decision tree is prone to overfitting, could we somehow combine multiple trees to create a more robust model? This question leads us to our next topic: <strong>Boosting / Bagging</strong>.</p> </div> </article>   </main> </body></html>